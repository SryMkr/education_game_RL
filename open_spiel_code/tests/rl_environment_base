"""Reinforcement Learning (RL) Environment for Open Spiel.
是结合game和rl_agent的一个环境，游戏和agent就在这个环境中交互
This module wraps Open Spiel Python interface providing an RL-friendly API. It
covers both turn-based and simultaneous move games. Interactions between agents
and the underlying game occur mostly through
---------------the `reset` and `step` methods,------------------------------
which return a `TimeStep` structure (see its docstrings for more info).
The following example illustrates the interaction dynamics. Consider a 2-player
Kuhn Poker (turn-based game). Agents have access to the `observations` (a dict)
field from `TimeSpec`, containing the following members:
 * `info_state`: list containing the game information state for each player. The
   size of the list always correspond to the number of players. E.g.:
   [[0, 1, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0]].
 * `legal_actions`: list containing legal action ID lists (one for each player).
   E.g.: [[0, 1], [0]], which corresponds to actions 0 and 1 being valid for
   player 0 (the 1st player) and action 0 being valid for player 1 (2nd player).
 * `current_player`: zero-based integer representing the player to make a move.

At each `step` call, the environment expects a singleton list with the action
(as it's a turn-based game), e.g.: [1]. This (zero-based) action must correspond
to the player specified at `current_player`. The game (which is at decision
node) will process the action and take as many steps necessary to cover chance
nodes, halting at a new decision or final node. Finally, a new `TimeStep`is
returned to the agent.

Simultaneous-move games follow analogous dynamics. The only differences is the
environment expects a list of actions, one per player. Note the `current_player`
field is "irrelevant" here, admitting a constant value defined in spiel.h, which
defaults to -2 (module level constant `SIMULTANEOUS_PLAYER_ID`).
See open_spiel/python/examples/rl_example.py for example usages.
"""

import collections  # 定义一个特殊的容器
import enum  # 枚举类型
from absl import logging  # 导入日志
import numpy as np
import pyspiel

SIMULTANEOUS_PLAYER_ID = pyspiel.PlayerId.SIMULTANEOUS  # 如果是SIMULTANEOUS游戏，则游戏玩家的ID是 -2


# 定义一个时间步的类
class TimeStep(collections.namedtuple("TimeStep", ["observations", "rewards", "discounts", "step_type"])):
    """Returned with every call to `step` and `reset`.
  A `TimeStep` contains the data emitted by a game at each step of interaction.
  A `TimeStep` holds an `observation` (list of dicts, one per player),
  associated lists of `rewards`, `discounts` and a `step_type`.
  The first `TimeStep` in a sequence will have `StepType.FIRST`. The final
  `TimeStep` will have `StepType.LAST`. All other `TimeStep`s in a sequence will
  have `StepType.MID.
  # first，mid, last 主要是用来知道游戏的状态
  Attributes:
    observations: a list of dicts containing observations per player.
    rewards: A list of scalars (one per player), or `None` if `step_type` is
      `StepType.FIRST`, i.e. at the start of a sequence.
    discounts: A list of discount values in the range `[0, 1]` (one per player),
      or `None` if `step_type` is `StepType.FIRST`.
    step_type: A `StepType` enum value.
  """
    __slots__ = ()

    def first(self):  # 游戏的初始状态
        return self.step_type == StepType.FIRST

    def mid(self):  # 游戏的中间状态
        return self.step_type == StepType.MID

    def last(self):  # 游戏的最终状态
        return self.step_type == StepType.LAST

    def is_simultaneous_move(self):
        return self.observations["current_player"] == SIMULTANEOUS_PLAYER_ID

    def current_player(self):
        return self.observations["current_player"]


# 定义一个时间步的类型
class StepType(enum.Enum):
    """Defines the status of a `TimeStep` within a sequence."""

    FIRST = 0  # Denotes the first `TimeStep` in a sequence. 游戏的初始状态
    MID = 1  # Denotes any `TimeStep` in a sequence that is not FIRST or LAST.  游戏的中间状态
    LAST = 2  # Denotes the last `TimeStep` in a sequence.  游戏的最终状态

    def first(self):
        return self is StepType.FIRST

    def mid(self):
        return self is StepType.MID

    def last(self):
        return self is StepType.LAST


# Global pyspiel members，就是获得open-spiel中的所有游戏
def registered_games():
    return pyspiel.registered_games()


# 返回机会玩家的采取的【动作，概率】
class ChanceEventSampler(object):
    """Default sampler for external chance events."""

    def __init__(self, seed=None):
        self.seed(seed)

    def seed(self, seed=None):
        self._rng = np.random.RandomState(seed)

    def __call__(self, state):
        """Sample a chance event in the given state."""
        actions, probs = zip(*state.chance_outcomes())   # 解压机会玩家所有可能存在的【动作，概率】
        # 选择一个动作返回
        return self._rng.choice(actions, p=probs)


# 定义用哪种信息类型，如果是完全信息就是information，如果是非完全信息则是observation
class ObservationType(enum.Enum):
    """Defines what kind of observation to use."""
    OBSERVATION = 0  # Use observation_tensor
    INFORMATION_STATE = 1  # Use information_state_tensor


# agent与环境交互的时候主要就是使用这个类，实现这个类的方法或者调用
class Environment(object):
    """Open Spiel reinforcement learning environment class."""

    def __init__(self,
                 game,
                 discount=1.0,  # 就是对未来可能的收益打折扣，计算期望的时候使用
                 chance_event_sampler=None,  # 机会玩家
                 observation_type=None,  # 游戏信息类型，默认是完全信息的游戏
                 include_full_state=False,  # 没啥哟个
                 mfg_distribution=None,  # mfg和我们没啥关系
                 mfg_population=None,  # mfg和我们没啥关系
                 enable_legality_check=False,  # 能够接受的合法的动作
                 **kwargs):  # 其他的需要传入的参数
        """Constructor.
    Args:
      game: [string, pyspiel.Game] Open Spiel game name or game instance.
      discount: float, discount used in non-initial steps. Defaults to 1.0.
      chance_event_sampler: optional object with `sample_external_events` method
        to sample chance events.
      observation_type: what kind of observation to use. If not specified, will
        default to INFORMATION_STATE unless the game doesn't provide it.
      include_full_state: whether or not to include the full serialized
        OpenSpiel state in the observations (sometimes useful for debugging).
      mfg_distribution: the distribution over states if the game is a mean field
        game.
      mfg_population: The Mean Field Game population to consider.
      enable_legality_check: Check the legality of the move before stepping.
      **kwargs: dict, additional settings passed to the Open Spiel game.
    """
        self._chance_event_sampler = chance_event_sampler or ChanceEventSampler()  # 机会玩家
        self._include_full_state = include_full_state  # 获取全部的state
        self._mfg_distribution = mfg_distribution
        self._mfg_population = mfg_population
        self._enable_legality_check = enable_legality_check  # 合法动作

        # 如果游戏有其他的参数，要将其他的参数接受进来
        if isinstance(game, str):
            if kwargs:
                game_settings = {key: val for (key, val) in kwargs.items()}
                logging.info("Using game settings: %s", game_settings)
                self._game = pyspiel.load_game(game, game_settings)
            else:
                logging.info("Using game string: %s", game)
                self._game = pyspiel.load_game(game)
        else:  # pyspiel.Game or API-compatible object.
            logging.info("Using game instance: %s", game.get_type().short_name)
            self._game = game

        self._num_players = self._game.num_players()  # 游戏玩家
        self._state = None  # 初始化为0
        self._should_reset = True  # 是否reset

        # Discount returned at non-initial steps.
        self._discounts = [discount] * self._num_players  # 每个玩家的折扣

        # Determine what observation type to use.
        if observation_type is None:
            if self._game.get_type().provides_information_state_tensor:
                observation_type = ObservationType.INFORMATION_STATE
            else:
                observation_type = ObservationType.OBSERVATION

        # Check the requested observation type is supported.
        if observation_type == ObservationType.OBSERVATION:
            if not self._game.get_type().provides_observation_tensor:
                raise ValueError(f"observation_tensor not supported by {game}")
        elif observation_type == ObservationType.INFORMATION_STATE:
            if not self._game.get_type().provides_information_state_tensor:
                raise ValueError(f"information_state_tensor not supported by {game}")
        self._use_observation = (observation_type == ObservationType.OBSERVATION)

        if self._game.get_type().dynamics == pyspiel.GameType.Dynamics.MEAN_FIELD:
            assert mfg_distribution is not None
            assert mfg_population is not None
            assert 0 <= mfg_population < self._num_players

    def seed(self, seed=None):
        self._chance_event_sampler.seed(seed)

    # 仅仅是获得当前这个step的一些信息，不做任何操作
    def get_time_step(self):
        """Returns a `TimeStep` without updating the environment.
    Returns:
      A `TimeStep` namedtuple containing:
        observation: list of dicts containing one observations per player, each
          corresponding to `observation_spec()`.
        reward: list of rewards at this timestep, or None if step_type is
          `StepType.FIRST`.
        discount: list of discounts in the range [0, 1], or None if step_type is
          `StepType.FIRST`.
        step_type: A `StepType` value.
    """
        # 一个observation中有的所有信息
        observations = {
            "info_state": [],
            "legal_actions": [],
            "current_player": [],
            "serialized_state": []
        }
        rewards = []
        step_type = StepType.LAST if self._state.is_terminal() else StepType.MID
        self._should_reset = step_type == StepType.LAST
        # 要了解以下的信息，必须首先了解state里面的方法，因为里面全部都有
        cur_rewards = self._state.rewards()
        for player_id in range(self.num_players):
            rewards.append(cur_rewards[player_id])
            observations["info_state"].append(
                self._state.observation_tensor(player_id) if self._use_observation
                else self._state.information_state_tensor(player_id))

            observations["legal_actions"].append(self._state.legal_actions(player_id))
        observations["current_player"] = self._state.current_player()
        discounts = self._discounts
        if step_type == StepType.LAST:
            # When the game is in a terminal state set the discount to 0.
            discounts = [0. for _ in discounts]

        if self._include_full_state:
            observations["serialized_state"] = pyspiel.serialize_game_and_state(
                self._game, self._state)

        # For gym environments
        if hasattr(self._state, "last_info"):
            observations["info"] = self._state.last_info
        # 反正记住了一个time step中有observation，reward，discount，step-type就行了
        return TimeStep(
            observations=observations,
            rewards=rewards,
            discounts=discounts,
            step_type=step_type)

    # 检查动作是否合法，不返回任何值。将turn_based 和 non_turn_based分开了
    def _check_legality(self, actions):
        if self.is_turn_based:
            legal_actions = self._state.legal_actions()
            if actions[0] not in legal_actions:
                raise RuntimeError(f"step() called on illegal action {actions[0]}")
        else:
            for p in range(len(actions)):
                legal_actions = self._state.legal_actions(p)
                if legal_actions and actions[p] not in legal_actions:
                    raise RuntimeError(f"step() by player {p} called on illegal " +
                                       f"action: {actions[p]}")

    #  这个方法才是环境中的重中之重，以上都是些辅助的东西不记住也没关系，只接受动作
    def step(self, actions):
        """Updates the environment according to `actions` and returns a `TimeStep`.
    If the environment returned a `TimeStep` with `StepType.LAST` at the
    previous step, this call to `step` will start a new sequence and `actions`
    will be ignored.
    This method will also start a new sequence if called after the environment
    has been constructed and `reset` has not been called. Again, in this case
    `actions` will be ignored.
    Args:
      actions: a list containing one action per player, following specifications
        defined in `action_spec()`.
    Returns:
      A `TimeStep` namedtuple containing:
        observation: list of dicts containing one observations per player, each
          corresponding to `observation_spec()`.
        reward: list of rewards at this timestep, or None if step_type is
          `StepType.FIRST`.
        discount: list of discounts in the range [0, 1], or None if step_type is
          `StepType.FIRST`.
        step_type: A `StepType` value.
    """
        assert len(actions) == self.num_actions_per_step, (
            "Invalid number of actions! Expected {}".format(
                self.num_actions_per_step))
        if self._should_reset:
            return self.reset()

        if self._enable_legality_check:
            self._check_legality(actions)

        if self.is_turn_based:
            self._state.apply_action(actions[0])
        else:
            self._state.apply_actions(actions)
        self._sample_external_events()

        return self.get_time_step()

    def reset(self):
        """Starts a new sequence and returns the first `TimeStep` of this sequence.
    Returns:
      A `TimeStep` namedtuple containing:
        observations: list of dicts containing one observations per player, each
          corresponding to `observation_spec()`.
        rewards: list of rewards at this timestep, or None if step_type is
          `StepType.FIRST`.
        discounts: list of discounts in the range [0, 1], or None if step_type
          is `StepType.FIRST`.
        step_type: A `StepType` value.
    """
        self._should_reset = False
        if self._game.get_type(
        ).dynamics == pyspiel.GameType.Dynamics.MEAN_FIELD and self._num_players > 1:
            self._state = self._game.new_initial_state_for_population(
                self._mfg_population)
        else:
            self._state = self._game.new_initial_state()  # 直接初始化
        self._sample_external_events()  # 外部环境的初始化
        # 将所有的参数都清空
        observations = {
            "info_state": [],
            "legal_actions": [],
            "current_player": [],
            "serialized_state": []
        }
        for player_id in range(self.num_players):
            observations["info_state"].append(
                self._state.observation_tensor(player_id) if self._use_observation
                else self._state.information_state_tensor(player_id))
            observations["legal_actions"].append(self._state.legal_actions(player_id))
        observations["current_player"] = self._state.current_player()

        if self._include_full_state:
            observations["serialized_state"] = pyspiel.serialize_game_and_state(
                self._game, self._state)
        # 和之前的step一样，
        return TimeStep(
            observations=observations,
            rewards=None,
            discounts=None,
            step_type=StepType.FIRST)

    #  看来主要是将机会玩家也重置了
    def _sample_external_events(self):
        """Sample chance events until we get to a decision node."""
        while self._state.is_chance_node() or (self._state.current_player()
                                               == pyspiel.PlayerId.MEAN_FIELD):
            if self._state.is_chance_node():
                outcome = self._chance_event_sampler(self._state)
                self._state.apply_action(outcome)
            if self._state.current_player() == pyspiel.PlayerId.MEAN_FIELD:
                dist_to_register = self._state.distribution_support()
                dist = [
                    self._mfg_distribution.value_str(str_state, default_value=0.0)
                    for str_state in dist_to_register
                ]
                self._state.update_distribution(dist)

    # 就是observation中具体包含那些信息
    def observation_spec(self):
        """Defines the observation per player provided by the environment.
    Each dict member will contain its expected structure and shape. E.g.: for
    Kuhn Poker {"info_state": (6,), "legal_actions": (2,), "current_player": (),
                "serialized_state": ()}
    Returns:
      A specification dict describing the observation fields and shapes.
    """
        return dict(
            info_state=tuple([
                self._game.observation_tensor_size() if self._use_observation else
                self._game.information_state_tensor_size()
            ]),
            legal_actions=(self._game.num_distinct_actions(),),
            current_player=(),
            serialized_state=(),
        )

    # 就是action中具体包含那些信息
    def action_spec(self):
        """Defines per player action specifications.
    Specifications include action boundaries and their data type.
    E.g.: for Kuhn Poker {"num_actions": 2, "min": 0, "max":1, "dtype": int}
    Returns:
      A specification dict containing per player action properties.
    """
        return dict(
            num_actions=self._game.num_distinct_actions(),
            min=0,
            max=self._game.num_distinct_actions() - 1,
            dtype=int,
        )

    # Environment properties
    @property
    def use_observation(self):
        """Returns whether the environment is using the game's observation.
    If false, it is using the game's information state. 看使用的是哪种observation
    """
        return self._use_observation

    # Game properties 返回游戏的名字
    @property
    def name(self):
        return self._game.get_type().short_name

    # 玩家的个数，这都是环境的属性可以直接调用
    @property
    def num_players(self):
        return self._game.num_players()

    # 玩家的可采取的动作，这都是环境的属性可以直接调用
    @property
    def num_actions_per_step(self):
        return 1 if self.is_turn_based else self.num_players

    # New RL calls for more advanced use cases (e.g. search + RL). 是否是基于turn的
    @property
    def is_turn_based(self):
        return ((self._game.get_type().dynamics
                 == pyspiel.GameType.Dynamics.SEQUENTIAL) or
                (self._game.get_type().dynamics
                 == pyspiel.GameType.Dynamics.MEAN_FIELD))

    # 游戏最长的步长
    @property
    def max_game_length(self):
        return self._game.max_game_length()

    # 是不是机会玩家
    @property
    def is_chance_node(self):
        return self._state.is_chance_node()

    # 返回这个游戏
    @property
    def game(self):
        return self._game

    # 更新游戏状态，手动更新
    def set_state(self, new_state):
        """Updates the game state."""
        assert new_state.get_game() == self.game, (
            "State must have been created by the same game.")
        self._state = new_state

    # 获得当前游戏状态
    @property
    def get_state(self):
        return self._state

    @property
    def mfg_distribution(self):
        return self._mfg_distribution

    def update_mfg_distribution(self, mfg_distribution):
        """Updates the distribution over the states of the mean field game."""
        assert (
                self._game.get_type().dynamics == pyspiel.GameType.Dynamics.MEAN_FIELD)
        self._mfg_distribution = mfg_distribution
