"""
参考https://github.com/deepmind/open_spiel/blob/master/open_spiel/python/rl_environment.py

"Reinforcement Learning (RL) Environment for Open Spiel. 是Open Spiel中的python的API。其中包括turn-based，和simultaneous游戏。agents和environment的交互主要通过reset和step方法交互，并且返回一个TimeStep结构
一个TimeStep结构包括了四个字段【observation, rewards, discounts, step_type】

agents可以通过TimeStep结构体中的observations字段来访问观测信息。observations中的的结构是一个字典--字段名字：List, observations字段包括以下的成员:
`info_state`: a list containing the game information state for each player. The size of the list always corresponds to the number of players. E.g.:[[0, 1, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0]].
核心理念就是每个玩家都有自己可观测的信息，所以调用的时候直接访问observations字段中的info_state的成员，使用索引可以直接观测到该玩家能够看到的信息

`legal_actions`: a list containing legal action ID lists. The size of the list always corresponds to the number of players. E.g.: [[0, 1], [0]], which corresponds to actions 0 and 1 being valid for
核心理念是每个玩家一开始都有自己的动作空间，随着游戏的进行，要改变可选择的动作空间，最基本的方法就是直接将合法动作ID从玩家对应的List中移除。

`current_player`: zero-based integer representing the player to make a move.一个从0开始的整数，表示了下一个需要做动作的玩家。

每次调用step方法时，environment期望可以得到一个包含动作ID的单例列表（回合制游戏中列表中只有一个数据）e.g.: [1]. 这个动作必须和'current_player'字段的玩家对应。游戏将处理这个动作，并且处理必要的其他的信息，直到需要agents做决策的节点
紧接着返回一个新的`TimeStep`。

对于simultaneous游戏和turn-based的游戏差不多。唯一的区别是，接受的动作列表不再只有一个值，而是一个动作列表。在这种情况下，`current_player`将不再重要。在open-spiel中将用-2这个常量代表current_player

See open_spiel/python/examples/rl_example.py for example usages.

总的理解是：环境对接受的动作做出反应，而环境中包括所有的信息。并且在初始化的时候对于每一个agent都已经定义好了其可以看到的信息，合法动作空间的，以及当前的玩家，奖励，折扣，还有step的类型。agent通过观察环境返回的TimeStep,根据自己的策略选择
动作，并返回给environment。直到整个交互结束。

"""
import collections  # 定义一个特殊的容器
import enum  # 枚举类型
from absl import logging  # 导入日志，主要是做测试使用
import numpy as np
import pyspiel

SIMULTANEOUS_PLAYER_ID = pyspiel.PlayerId.SIMULTANEOUS  # 如果是SIMULTANEOUS游戏，则游戏玩家的ID是 -2

# 定义一个时间步的类
class TimeStep(collections.namedtuple("TimeStep", ["observations", "rewards", "discounts", "step_type"])):
    """ TimeStep这个类继承了一个具名元组，其实可以简单理解为这个类的属性。并且不能再添加任何其他的属性。只能定义新的方法，后面的__slots__ = () 也是为了保证这个类不接受其他的属性。
        每次调用`step` and `reset`方法都会返回这样的一个TimeStep的类结构体。
         
         TimeStep 包含了游戏在每次交互步骤中产生的数据。
         一个TimeStep包含了四个成员，就是前面定义的四个成员。在observation中包含了三个成员{infor_state:[[],[]...], legal_actions:[[],[]...], current_player: interger}

         在一个序列中，第一个 TimeStep 将具有 StepType.FIRST。最后一个 TimeStep 将具有 StepType.LAST。序列中的所有其他 TimeStep 将具有 StepType.MID。因为在一个trajectory中会返回多个TimeStep,其实就是定义了三个方法
         First可以返回游戏开始的最初状态，Last可以访问游戏的中间状态，Mid可以访问游戏的中间状态。

          四个属性的介绍：
             observations: a list of dicts containing observations per player. 每个玩家都有一个observation
             rewards: A list of scalars (one per player), or `None` if `step_type` is `StepType.FIRST`, i.e. at the start of a sequence.一组标量列表，分别代表每个玩家的奖励
             discounts: A list of discount values in the range `[0, 1]` (one per player) or `None` if `step_type` is `StepType.FIRST`.列表中的浮点数在【0，1】之间，每个玩家都有一个折扣
             step_type: A `StepType` enum value.枚举类型
           """

    __slots__ = () #保证这个类不接受其他的属性。

    def first(self):  # 游戏的初始状态
        return self.step_type == StepType.FIRST

    def mid(self):  # 游戏的中间状态
        return self.step_type == StepType.MID

    def last(self):  # 游戏的最终状态
        return self.step_type == StepType.LAST

    # 如果是simultaneous游戏，将observation字段中的'current_player'这个key的值变为-2
    def is_simultaneous_move(self):
        return self.observations["current_player"] == SIMULTANEOUS_PLAYER_ID

   # 读取observation字段中的'current_player'这个key的值
    def current_player(self):
        return self.observations["current_player"]


# 定义时间步的类型，枚举类型，继承枚举类型，也可以定义一些方法
class StepType(enum.Enum):
    """ Defines the status of a `TimeStep` within a sequence."""

    FIRST = 0  # Denotes the first `TimeStep` in a sequence. 游戏的初始状态
    MID = 1  # Denotes any `TimeStep` in a sequence that is not FIRST or LAST.  游戏的中间状态
    LAST = 2  # Denotes the last `TimeStep` in a sequence.  游戏的最终状态

    def first(self):
        return self is StepType.FIRST

    def mid(self):
        return self is StepType.MID

    def last(self):
        return self is StepType.LAST


# Global pyspiel members，就是获得open-spiel中的所有游戏的名字
def registered_games():
    return pyspiel.registered_games()


# 返回机会玩家的采取的【动作，概率】
class ChanceEventSampler(object):
    """ Default sampler for external chance events."""

    def __init__(self, seed=None):
        self.seed(seed) # 随机数种子

    def seed(self, seed=None):
        self._rng = np.random.RandomState(seed) # 生成伪随机数

    def __call__(self, state): # __call__是直接可以被调用，相当于默认被调用的类的方法
        """ Sample a chance event in the given state."""
        actions, probs = zip(*state.chance_outcomes())   # 解压机会玩家所有可能存在的【动作，概率】
        # 选择一个动作返回
        return self._rng.choice(actions, p=probs) # 按照概率选择一个动作


# 定义用哪种信息类型，如果是完全信息就是information，如果是非完全信息则是observation
class ObservationType(enum.Enum):
    """ Defines what kind of observation to use."""
    OBSERVATION = 0  # Use observation_tensor
    INFORMATION_STATE = 1  # Use information_state_tensor


# agent与环境交互的时候主要就是使用这个类，实现这个类的方法或者调用
class Environment(object):
    """ Open Spiel reinforcement learning environment class."""

    def __init__(self,
                 game,
                 discount=1.0,  # 就是对未来可能的收益打折扣，计算期望的时候使用
                 chance_event_sampler=None,  # 机会玩家
                 observation_type=None,  # 游戏信息类型，默认是完全信息的游戏
                 include_full_state=False,  # 没啥哟个
                 mfg_distribution=None,  # mfg和我们没啥关系
                 mfg_population=None,  # mfg和我们没啥关系
                 enable_legality_check=False,  # 能够接受的合法的动作
                 **kwargs):  # 其他的需要传入的参数
        """ Constructor.
    Args:
      game: [string, pyspiel.Game] Open Spiel game name or game instance.
      discount: float, discount used in non-initial steps. Defaults to 1.0.
      chance_event_sampler: optional object with `sample_external_events` method
        to sample chance events.
      observation_type: what kind of observation to use. If not specified, will
        default to INFORMATION_STATE unless the game doesn't provide it.
      include_full_state: whether or not to include the full serialized
        OpenSpiel state in the observations (sometimes useful for debugging).
      mfg_distribution: the distribution over states if the game is a mean-field
        game.
      mfg_population: The Mean Field Game population to consider.
      enable_legality_check: Check the legality of the move before stepping.
      **kwargs: dict, additional settings passed to the Open Spiel game.
    """
        self._chance_event_sampler = chance_event_sampler or ChanceEventSampler()  # 机会玩家
        self._include_full_state = include_full_state  # 获取全部的state
        self._mfg_distribution = mfg_distribution
        self._mfg_population = mfg_population
        self._enable_legality_check = enable_legality_check  # 合法动作

        # 如果游戏有其他的参数，要将其他的参数接受进来
        if isinstance(game, str):
            if kwargs:
                game_settings = {key: val for (key, val) in kwargs.items()}
                logging.info("Using game settings: %s", game_settings)
                self._game = pyspiel.load_game(game, game_settings)
            else:
                logging.info("Using game string: %s", game)
                self._game = pyspiel.load_game(game)
        else:  # pyspiel.Game or API-compatible object.
            logging.info("Using game instance: %s", game.get_type().short_name)
            self._game = game

        self._num_players = self._game.num_players()  # 游戏玩家
        self._state = None  # 要返回给agent做决策，是一个TimeStep
        self._should_reset = True  # 是否reset

        # Discount returned at non-initial steps.
        self._discounts = [discount] * self._num_players  # 每个玩家都有一个折扣，得到的每个玩家的折扣列表

        # Determine what observation type to use.
        if observation_type is None:
            if self._game.get_type().provides_information_state_tensor:
                observation_type = ObservationType.INFORMATION_STATE
            else:
                observation_type = ObservationType.OBSERVATION

        # Check the requested observation type is supported.
        if observation_type == ObservationType.OBSERVATION:
            if not self._game.get_type().provides_observation_tensor:
                raise ValueError(f"observation_tensor not supported by {game}")
        elif observation_type == ObservationType.INFORMATION_STATE:
            if not self._game.get_type().provides_information_state_tensor:
                raise ValueError(f"information_state_tensor not supported by {game}")
        self._use_observation = (observation_type == ObservationType.OBSERVATION)

        if self._game.get_type().dynamics == pyspiel.GameType.Dynamics.MEAN_FIELD:
            assert mfg_distribution is not None
            assert mfg_population is not None
            assert 0 <= mfg_population < self._num_players

    def seed(self, seed=None):
        self._chance_event_sampler.seed(seed)

    # 捕捉一个Timestep的所有信息，通过访问state捕捉所有的必要信息，也就是说环境从初始状态变化的过程
    def get_time_step(self):
        """ Returns a `TimeStep` without updating the environment.
    Returns:
      A `TimeStep` namedtuple containing:
        observation: list of dicts containing one observations per player, each
          corresponding to `observation_spec()`.
        reward: list of rewards at this timestep, or None if step_type is
          `StepType.FIRST`.
        discount: list of discounts in the range [0, 1], or None if step_type is
          `StepType.FIRST`.
        step_type: A `StepType` value.
    """
        # 一个observation中有的所有信息
        observations = {
            "info_state": [],
            "legal_actions": [],
            "current_player": [],
            "serialized_state": []
        }
        rewards = []
        step_type = StepType.LAST if self._state.is_terminal() else StepType.MID
        self._should_reset = step_type == StepType.LAST # 当游戏结束的时候，需要reset游戏
        # 要了解以下的信息，必须首先了解state里面的方法，因为里面全部都有
        cur_rewards = self._state.rewards()
        for player_id in range(self.num_players):
            rewards.append(cur_rewards[player_id])
            observations["info_state"].append(
                self._state.observation_tensor(player_id) if self._use_observation
                else self._state.information_state_tensor(player_id))

            observations["legal_actions"].append(self._state.legal_actions(player_id))
        observations["current_player"] = self._state.current_player()
        discounts = self._discounts
        if step_type == StepType.LAST:
            # When the game is in a terminal state set the discount to 0.
            discounts = [0. for _ in discounts]
         # 保存整个state的所有信息
        if self._include_full_state:
            observations["serialized_state"] = pyspiel.serialize_game_and_state(
                self._game, self._state)

        # For gym environments
        if hasattr(self._state, "last_info"):
            observations["info"] = self._state.last_info
        # 反正记住了一个time step中有observation，reward，discount，step-type就行了
        return TimeStep(
            observations=observations,
            rewards=rewards,
            discounts=discounts,
            step_type=step_type)

    # 检查动作是否合法，不返回任何值。将turn_based 和 non_turn_based分开了，我们不做检测，所以没什么用
    def _check_legality(self, actions):
        if self.is_turn_based:
            legal_actions = self._state.legal_actions()
            if actions[0] not in legal_actions:
                raise RuntimeError(f"step() called on illegal action {actions[0]}")
        else:
            for p in range(len(actions)):
                legal_actions = self._state.legal_actions(p)
                if legal_actions and actions[p] not in legal_actions:
                    raise RuntimeError(f"step() by player {p} called on illegal " +
                                       f"action: {actions[p]}")

    #  这个方法才是环境中的重中之重，接受动作信息，对环境的初始化state做出改变，所以定义一个state是辅助获得env中返回的信息状态
    def step(self, actions):
        """ Updates the environment according to `actions` and returns a `TimeStep`.

    If the environment returned a `TimeStep` with `StepType.LAST` at the previous step, this call to `step` will start a new sequence,  and `actions` will be ignored.
    This method will also start a new sequence if called after the environment has been constructed and `reset` has not been called. Again, in this case, `actions` will be ignored.
    Args:
      actions: a list containing one action per player, following specifications defined in `action_spec()`.
    Returns:
      A `TimeStep` namedtuple containing:
        observation: list of dicts containing one observations per player, each
          corresponding to `observation_spec()`.
        reward: list of rewards at this timestep, or None if step_type is
          `StepType.FIRST`.
        discount: list of discounts in the range [0, 1], or None if step_type is
          `StepType.FIRST`.
        step_type: A `StepType` value.
    """
        assert len(actions) == self.num_actions_per_step, (
            "Invalid number of actions! Expected {}".format(
                self.num_actions_per_step))
        if self._should_reset:
            return self.reset()

        if self._enable_legality_check:
            self._check_legality(actions)

        if self.is_turn_based:
            self._state.apply_action(actions[0])
        else:
            self._state.apply_actions(actions)
        self._sample_external_events()
        return self.get_time_step()

    def reset(self):
        """ Starts a new sequence and returns the first `TimeStep` of this sequence.
    Returns:
      A `TimeStep` namedtuple containing:
        observations: list of dicts containing one observations per player, each
          corresponding to `observation_spec()`.
        rewards: list of rewards at this timestep, or None if step_type is
          `StepType.FIRST`.
        discounts: list of discounts in the range [0, 1], or None if step_type
          is `StepType.FIRST`.
        step_type: A `StepType` value.
    """
        self._should_reset = False
        if self._game.get_type(
        ).dynamics == pyspiel.GameType.Dynamics.MEAN_FIELD and self._num_players > 1:
            self._state = self._game.new_initial_state_for_population(
                self._mfg_population)
        else:
            self._state = self._game.new_initial_state()  # 直接初始化
        self._sample_external_events()  # 外部环境的初始化
        # 将所有的参数都清空
        observations = {
            "info_state": [],
            "legal_actions": [],
            "current_player": [],
            "serialized_state": []
        }
        for player_id in range(self.num_players):
            observations["info_state"].append(
                self._state.observation_tensor(player_id) if self._use_observation
                else self._state.information_state_tensor(player_id))
            observations["legal_actions"].append(self._state.legal_actions(player_id))
        observations["current_player"] = self._state.current_player()

        if self._include_full_state:
            observations["serialized_state"] = pyspiel.serialize_game_and_state(
                self._game, self._state)
        # 和之前的step一样，
        return TimeStep(
            observations=observations,
            rewards=None,
            discounts=None,
            step_type=StepType.FIRST)

    #  看来主要是将机会玩家也重置了
    def _sample_external_events(self):
        """ Sample chance events until we get to a decision node."""
        while self._state.is_chance_node() or (self._state.current_player()
                                               == pyspiel.PlayerId.MEAN_FIELD):
            if self._state.is_chance_node():
                outcome = self._chance_event_sampler(self._state)
                self._state.apply_action(outcome)
            if self._state.current_player() == pyspiel.PlayerId.MEAN_FIELD:
                dist_to_register = self._state.distribution_support()
                dist = [
                    self._mfg_distribution.value_str(str_state, default_value=0.0)
                    for str_state in dist_to_register
                ]
                self._state.update_distribution(dist)

    # 就是observation中具体包含那些信息，是用向量表示的observation，定义一个向量
    def observation_spec(self):
        """Defines the observation per player provided by the environment.
    Each dict member will contain its expected structure and shape. E.g.: for
    Kuhn Poker {"info_state": (6,), "legal_actions": (2,), "current_player": (),
                "serialized_state": ()}
    Returns:
      A specification dict describing the observation fields and shapes.
    """
        return dict(
            info_state=tuple([
                self._game.observation_tensor_size() if self._use_observation else
                self._game.information_state_tensor_size()
            ]),
            legal_actions=(self._game.num_distinct_actions(),),
            current_player=(),
            serialized_state=(),
        )

    # 就是action中具体包含那些信息
    def action_spec(self):
        """ Defines per player action specifications.
    Specifications include action boundaries and their data type.
    E.g.: for Kuhn Poker {"num_actions": 2, "min": 0, "max":1, "dtype": int}
    Returns:
      A specification dict containing per player action properties.
    """
        return dict(
            num_actions=self._game.num_distinct_actions(),
            min=0,
            max=self._game.num_distinct_actions() - 1,
            dtype=int,
        )

    # Environment properties
    @property
    def use_observation(self):
        """ Returns whether the environment is using the game's observation.
    If false, it is using the game's information state. 看使用的是哪种observation
    """
        return self._use_observation

    # Game properties 返回游戏的名字
    @property
    def name(self):
        return self._game.get_type().short_name

    # 玩家的个数，这都是环境的属性可以直接调用
    @property
    def num_players(self):
        return self._game.num_players()

    # 玩家的可采取的动作，这都是环境的属性可以直接调用
    @property
    def num_actions_per_step(self):
        return 1 if self.is_turn_based else self.num_players

    # New RL calls for more advanced use cases (e.g. search + RL). 是否是基于turn的
    @property
    def is_turn_based(self):
        return ((self._game.get_type().dynamics
                 == pyspiel.GameType.Dynamics.SEQUENTIAL) or
                (self._game.get_type().dynamics
                 == pyspiel.GameType.Dynamics.MEAN_FIELD))

    # 游戏最长的步长
    @property
    def max_game_length(self):
        return self._game.max_game_length()

    # 是不是机会玩家
    @property
    def is_chance_node(self):
        return self._state.is_chance_node()

    # 返回这个游戏
    @property
    def game(self):
        return self._game

    # 更新游戏状态，手动更新
    def set_state(self, new_state):
        """ Updates the game state."""
        assert new_state.get_game() == self.game, (
            "State must have been created by the same game.")
        self._state = new_state

    # 获得当前游戏状态
    @property
    def get_state(self):
        return self._state

    @property
    def mfg_distribution(self):
        return self._mfg_distribution

    def update_mfg_distribution(self, mfg_distribution):
        """ Updates the distribution over the states of the mean-field game."""
        assert (
                self._game.get_type().dynamics == pyspiel.GameType.Dynamics.MEAN_FIELD)
        self._mfg_distribution = mfg_distribution
